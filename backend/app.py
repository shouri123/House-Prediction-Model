import os
import pandas as pd
import numpy as np
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import joblib
import logging
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Global Variables & Constants ---
model = None
REQUIRED_COLUMNS = [
    'longitude', 'latitude', 'housing_median_age', 'total_rooms',
    'total_bedrooms', 'population', 'households', 'median_income',
    'ocean_proximity'
]

OCEAN_PROXIMITY_OPTIONS = [
    '<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'
]

# --- Helper Functions ---

def load_model():
    """Loads the trained model from disk."""
    global model
    model_path = os.path.join(os.path.dirname(__file__), '..', 'models', 'model.pkl')
    try:
        if os.path.exists(model_path):
            model = joblib.load(model_path)
            logger.info(f"Model loaded successfully from {model_path}")
            return True
        else:
            logger.error(f"Model file not found at {model_path}")
            return False
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        return False

def validate_input(df):
    """Validates that the input DataFrame contains all required columns."""
    missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]
    if missing_cols:
        return False, f"Missing required columns: {', '.join(missing_cols)}"
    return True, ""

def add_engineered_features(df):
    """Adds engineered features to the DataFrame."""
    df = df.copy()
    # Handle division by zero or missing values if necessary
    df['rooms_per_household'] = df['total_rooms'] / df['households']
    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']
    df['population_per_household'] = df['population'] / df['households']
    return df

def get_feature_importance():
    """Extracts feature importance from the model pipeline if available."""
    try:
        if model and hasattr(model, 'named_steps'):
            # Assuming 'model' step is the regressor (Ridge/LinearRegression)
            regressor = model.named_steps.get('model') or model.named_steps.get('regressor')
            if hasattr(regressor, 'coef_'):
                # This is a simplification. Ideally, we map names to coefs.
                # For now, return top absolute coefficients if we can't map names easily without preprocessor
                return list(regressor.coef_[:10]) 
    except Exception as e:
        logger.warning(f"Could not extract feature importance: {e}")
    return []

def estimate_confidence_intervals(predictions):
    """Generates dummy confidence intervals for demonstration."""
    # In a real scenario, this would use prediction intervals from the model
    margins = []
    for pred in predictions:
        margin = pred * 0.1  # 10% margin
        margins.append({
            'low': pred - margin,
            'high': pred + margin,
            'margin': margin
        })
    return margins

def detect_outliers(df, predictions):
    """Simple outlier detection based on Z-score of predictions."""
    # detailed implementation omitted for brevity/stability, returning safe defaults
    return [], 0, 0

def generate_insights(df, predictions, importance):
    """Generates text insights based on data."""
    insights = []
    
    # 1. Average Price
    avg_price = np.mean(predictions)
    insights.append({
        'type': 'info',
        'icon': 'ðŸ’°',
        'title': 'Average Prediction',
        'text': f"The average predicted house value is ${avg_price:,.2f}."
    })

    # 2. Price Range
    min_price = np.min(predictions)
    max_price = np.max(predictions)
    insights.append({
        'type': 'success',
        'icon': 'ðŸ“Š',
        'title': 'Price Range',
        'text': f"Properties range from ${min_price:,.0f} to ${max_price:,.0f}."
    })

    # 3. High Value Area Analysis (if ocean_proximity exists)
    if 'ocean_proximity' in df.columns:
        # Create a temp df for aggregation
        temp_df = df.copy()
        temp_df['predicted_price'] = predictions
        expensive_loc = temp_df.groupby('ocean_proximity')['predicted_price'].mean().idxmax()
        insights.append({
            'type': 'warning',
            'icon': 'ðŸŒŠ',
            'title': 'Prime Location',
            'text': f"Properties in '{expensive_loc}' tend to have the highest predicted values."
        })

    # 4. Income Correlation
    if 'median_income' in df.columns:
        correlation = np.corrcoef(df['median_income'], predictions)[0, 1]
        strength = "strong" if abs(correlation) > 0.7 else "moderate" if abs(correlation) > 0.4 else "weak"
        direction = "positive" if correlation > 0 else "negative"
        insights.append({
            'type': 'info',
            'icon': 'ðŸ“ˆ',
            'title': 'Income Factor',
            'text': f"There is a {strength} {direction} correlation ({correlation:.2f}) between income and house value."
        })

    return insights

def generate_graph_data(df):
    """Generates data for frontend charts."""
    graphs = {}

    # 1. Price Histogram
    # We need to bin the predicted prices
    if 'predicted_price' in df.columns:
        prices = df['predicted_price'].dropna()
        # Create histogram with 15 bins
        hist, bin_edges = np.histogram(prices, bins=15)
        
        # Format labels as ranges "100k-200k"
        labels = []
        for i in range(len(bin_edges) - 1):
            start = bin_edges[i] / 1000
            end = bin_edges[i+1] / 1000
            labels.append(f"${int(start)}k-${int(end)}k")
            
        graphs['histogram'] = {
            'labels': labels,
            'values': hist.tolist()
        }
        
        # Add summary stats
        graphs['summary_stats'] = {
            'total_properties': int(len(prices)),
            'avg_price': float(prices.mean()),
            'std_dev': float(prices.std()),
            'min_price': float(prices.min()),
            'max_price': float(prices.max())
        }

    # 2. Income vs Price Scatter
    if 'median_income' in df.columns and 'predicted_price' in df.columns:
        # Downsample if too many points to avoid lag
        if len(df) > 1000:
            sample_df = df.sample(1000, random_state=42)
        else:
            sample_df = df
            
        graphs['scatter'] = {
            'x': sample_df['median_income'].tolist(),
            'y': sample_df['predicted_price'].tolist()
        }

    return graphs

app = Flask(__name__, static_folder='../frontend/dist', static_url_path='')
CORS(app, resources={r"/*": {"origins": "*"}})

@app.before_request
def log_request_info():
    app.logger.info('Headers: %s', request.headers)
    app.logger.info('Body: %s', request.get_data())

@app.route('/')
def serve_frontend():
    return send_from_directory(app.static_folder, 'index.html')

@app.errorhandler(404)
def not_found(e):
    if request.path.startswith('/api/'):
        return jsonify({'error': 'Not found'}), 404
    return send_from_directory(app.static_folder, 'index.html')

@app.route('/api/info', methods=['GET'])
def api_info():
    """API Info endpoint."""
    return jsonify({
        'message': 'House Price Prediction API Server',
        'endpoints': ['/health', '/predict', '/predict-single', '/model-info'],
        'status': 'running'
    })

@app.route('/model-info', methods=['GET'])
def model_info():
    """Returns model metadata and feature importance."""
    if model is None:
        if not load_model():
            return jsonify({'error': 'Model is not available.'}), 503

    importance = get_feature_importance()
    return jsonify({
        'model_type': 'Ridge Regression',
        'feature_importance': importance,
        'ocean_proximity_options': OCEAN_PROXIMITY_OPTIONS,
        'required_columns': REQUIRED_COLUMNS
    })

@app.route('/predict-single', methods=['POST'])
def predict_single():
    """Predict price for a single property from JSON body."""
    if model is None:
        if not load_model():
            return jsonify({'error': 'Model is not available.'}), 503

    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        df = pd.DataFrame([data])

        # Validate
        is_valid, error_msg = validate_input(df)
        if not is_valid:
            return jsonify({'error': error_msg}), 400

        # Feature engineering
        df = add_engineered_features(df)

        # Predict
        prediction = model.predict(df)
        price = float(prediction[0])
        margin = price * 0.1  # 10% confidence interval

        return jsonify({
            'predicted_price': price,
            'confidence_low': price - margin,
            'confidence_high': price + margin,
            'margin': margin,
            'input': data
        })
    except Exception as e:
        logger.error(f"Error in single prediction: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/predict', methods=['POST'])
def predict():
    """Predict house prices from uploaded CSV/JSON file."""
    if model is None:
        if not load_model():
            return jsonify({'error': 'Model is not available.'}), 503

    if 'file' not in request.files:
        return jsonify({'error': 'No file part in the request'}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400

    try:
        # Read file
        if file.filename.endswith('.csv'):
            df = pd.read_csv(file)
        elif file.filename.endswith('.json'):
            df = pd.read_json(file)
        else:
            return jsonify({'error': 'Invalid file format. Upload CSV or JSON.'}), 400

        # Validate
        is_valid, error_msg = validate_input(df)
        if not is_valid:
            return jsonify({'error': error_msg}), 400

        # Check for actual values
        has_actual = 'median_house_value' in df.columns
        actual_values = df['median_house_value'].tolist() if has_actual else None

        # Feature engineering
        df = add_engineered_features(df)

        # Predict
        predictions = model.predict(df)
        df['predicted_price'] = predictions

        # Confidence intervals
        margins = estimate_confidence_intervals(predictions)

        # Outlier detection
        outlier_indices, outlier_lower, outlier_upper = detect_outliers(df, predictions)

        # Feature importance
        importance = get_feature_importance()

        # Smart insights
        insights = generate_insights(df, predictions, importance)

        # Graph data
        graph_data = generate_graph_data(df)

        # Model metrics (if actual values available)
        metrics = None
        predicted_vs_actual = None
        if has_actual:
            y_true = np.array(actual_values)
            y_pred = predictions
            metrics = {
                'mae': float(mean_absolute_error(y_true, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),
                'r2': float(r2_score(y_true, y_pred))
            }
            predicted_vs_actual = {
                'predicted': y_pred.tolist(),
                'actual': y_true.tolist()
            }
            # Additional insight about model accuracy
            insights.append({
                'type': 'info',
                'icon': 'ðŸŽ¯',
                'title': 'Model Accuracy',
                'text': f'RÂ² Score: {metrics["r2"]:.3f} | MAE: ${metrics["mae"]:,.0f} | RMSE: ${metrics["rmse"]:,.0f}'
            })

        # Clean NaN/inf values before JSON serialization
        df = df.replace([np.inf, -np.inf], np.nan).fillna(0)

        # Build response
        response_data = {
            'data': df.to_dict(orient='records'),
            'graphs': graph_data,
            'confidence_margins': margins,
            'outlier_indices': outlier_indices,
            'outlier_bounds': {'lower': outlier_lower, 'upper': outlier_upper},
            'feature_importance': importance,
            'insights': insights,
            'metrics': metrics,
            'predicted_vs_actual': predicted_vs_actual,
            'has_actual': has_actual,
            'message': 'Prediction successful'
        }

        return jsonify(response_data)

    except Exception as e:
        logger.error(f"Error processing request: {e}")
        return jsonify({'error': f"An error occurred: {str(e)}"}), 500

if __name__ == '__main__':
    debug_mode = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=debug_mode, host='0.0.0.0', port=port)
